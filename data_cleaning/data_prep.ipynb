{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and processing notebook\n",
    "\n",
    "Dataset descriptions and references: \n",
    "\n",
    "1) Yamagishi R, Kaneko H. Data from comprehensive analysis of nuclear localization signals. Data Brief. 2015 Dec 12;6:200-3. doi: 10.1016/j.dib.2015.11.064. PMID: 26862559; PMCID: PMC4707185.\n",
    "\n",
    "    UniProt IDs were extracted from a spreadsheet containing documented NLS sequences and the proteins in which they are located inside the supplementary materials of the paper cited above. These UniProt IDs were entered in the ID-mapping query on the UniProt website, which then generated a FASTA file containing all the full sequences of the proteins. We then used the SeqIO method inside the Biopython package to extract the sequences from the FASTA file and add them to a CSV. This CSV containing the full sequences was then merged with Pandas such that the proteins, organized by UniProt ID, contained both the NLS sequences and the full sequence.\n",
    "\n",
    "2)  https://services.healthtech.dtu.dk/services/DeepLoc-2.0/#\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from data_prep import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MT', 'SP_GPI', 'GPI', 'SP_TM', 'SP', 'NLS', 'TH', 'CH', 'TM',\n",
       "       'PTS', 'NES', 'NLS_NES', 'TM_MT', nan, 'SP_TM_PTS', 'SP_PTS'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/SortingSignalsSwissprot.csv\")\n",
    "df[\"Types\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with specific values in the 'Types' column\n",
    "values_to_drop = ['NLS_NES', 'TM_MT', np.nan, 'SP_TM_PTS', 'SP_PTS', 'SP_GPI', 'SP_TM']\n",
    "df = df[~df['Types'].isin(values_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=\"Kingdom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MT' 'GPI' 'SP' 'NLS' 'TH' 'CH' 'TM' 'PTS' 'NES']\n",
      "Types\n",
      "SP     768\n",
      "MT     236\n",
      "NLS    131\n",
      "PTS    125\n",
      "GPI     92\n",
      "CH      90\n",
      "NES     83\n",
      "TH      42\n",
      "TM      35\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Types\"].unique())\n",
    "\n",
    "type_counts = df['Types'].value_counts()\n",
    "print(type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming columns before combining two dataframes \n",
    "\n",
    "df2 = pd.read_csv(\"../data/finalized_complete_NLS_sequence_table.csv\")\n",
    "\n",
    "df2 = df2.rename(columns={'Sequence_full': \"Sequence\"})\n",
    "\n",
    "df2[\"Types\"] = \"NLS\"\n",
    "\n",
    "df2 = df2.rename(columns={\"UniProt ID\": \"ACC\"})\n",
    "\n",
    "df2['AnnotEncoded'] = df2.apply(generate_annotation, axis=1)\n",
    "\n",
    "df2 = df2.drop(columns=['Name', 'Begin','End','Length', 'Evidence', 'Sequence_nls', 'ECO code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the two data frames\n",
    "stacked_df = pd.concat([df, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove rows containing \"B\", \"U\", or \"X\" in the sequence\n",
    "cleaned_stacked_df = remove_bux(stacked_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the length column which shows lengths of the full protein sequences\n",
    "cleaned_stacked_df[\"Length\"] = cleaned_stacked_df[\"Sequence\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing duplicates \n",
    "df_cleaned = cleaned_stacked_df.drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_modified_df2.to_csv(\"cleaned_modified_df2.csv\",index=False)\n",
    "\n",
    "df_cleaned.to_csv(\"../data/finalized_df_cleaned.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
